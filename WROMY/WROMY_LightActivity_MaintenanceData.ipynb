{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebdb3cc",
   "metadata": {},
   "source": [
    "# Generate Data for Maintenance Activitiy based on WROMY LightSensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1819f3e0",
   "metadata": {},
   "source": [
    "Use the light sensors of WROMY to identify maintenance activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6eb9b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:07:53.981736Z",
     "start_time": "2022-08-16T15:07:53.979755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a828c5-8c84-4883-9f6e-80ae8d36274a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if os.uname().nodename == 'lighthouse':\n",
    "    root_path = '/home/andbro/'\n",
    "    data_path = '/home/andbro/kilauea-data/'\n",
    "    archive_path = '/home/andbro/freenas/'\n",
    "    bay_path = '/home/andbro/bay200/'\n",
    "elif os.uname().nodename == 'kilauea':\n",
    "    root_path = '/home/brotzer/'\n",
    "    data_path = '/import/kilauea-data/'\n",
    "    archive_path = '/import/freenas-ffb-01-data/'\n",
    "    bay_path = '/bay200/'\n",
    "elif os.uname().nodename == 'lin-ffb-01':\n",
    "    root_path = '/home/brotzer/'\n",
    "    data_path = '/import/kilauea-data/'\n",
    "    archive_path = '/import/freenas-ffb-01-data/'\n",
    "    bay_path = '/bay200/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45252103-c831-4a7f-a3f3-9e96c21e37aa",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c88a61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T15:07:54.003624Z",
     "start_time": "2022-08-16T15:07:53.982629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['date'] = \"2024-02-23\"\n",
    "\n",
    "config['year'] = config['date'][:4]\n",
    "\n",
    "config['threshold'] = 5 ## light threshold to classify as <light on> / <light off>\n",
    "\n",
    "config['path_to_LX_data'] = archive_path+\"romy_archive/\"\n",
    "\n",
    "config['path_to_outdata'] = archive_path+f\"romy_archive/{config['year']}/BW/WROMY/\"\n",
    "\n",
    "config['path_to_outlog'] = archive_path+f\"romy_autodata/{config['year']}/logfiles/\"\n",
    "\n",
    "config['path_to_figs'] = archive_path+f\"romy_plots/{config['year']}/logs/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4d5d6afe-51c0-4c9e-9e75-5f2c9cc804c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def __read_LX_files(date, path_to_files, threshold=5):\n",
    "\n",
    "    ## format date\n",
    "    fdate = str(date)[:10].replace(\"-\", \"\")\n",
    "\n",
    "    year = fdate[:4]\n",
    "\n",
    "    ## interate for all sensors of WROMY\n",
    "    counter = 0\n",
    "    for sensor in [1,4,5,6,7,8,9]:\n",
    "\n",
    "        ## assemble file name\n",
    "        filename = f'WS{sensor}_{fdate}.txt'\n",
    "\n",
    "        ## modify data path\n",
    "        datapath = f'{path_to_files}{year}/BW/WROMY/LX{sensor}.D/'\n",
    "\n",
    "        ## read files\n",
    "        if os.path.isfile(datapath+filename):\n",
    "            df = pd.read_csv(datapath+filename, names=['Date', 'Time', 'State'])\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(f\" -> {datapath+filename} does not exists!\")\n",
    "            continue\n",
    "\n",
    "        df['State'] = [1 if _val > threshold else 0 for _val in df['State'] ]\n",
    "\n",
    "        ## rename column properly by sensor number\n",
    "        df.rename(columns={\"State\":f\"WS{sensor}\"}, inplace=True)\n",
    "\n",
    "        ## prepend zeros for times in column Time and convert to string\n",
    "        df['Time'] = [str(_t).rjust(6, \"0\") for _t in df.Time]\n",
    "\n",
    "        ## convert Date to string\n",
    "        df['Date'] = df.Date.astype(str)\n",
    "\n",
    "        ## creat new datetime column\n",
    "        df['datetime'] = pd.to_datetime(df['Date'] + 'T' + df['Time'], format=\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "        ## set datetime column as index\n",
    "        df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "        ## resample to one minute rows and use maximum of values (=conserving ones)\n",
    "        df = df.resample('1min').max()\n",
    "\n",
    "        ## drop columns no longer needed\n",
    "        df.drop(columns=[\"Date\", \"Time\"], inplace=True)\n",
    "\n",
    "        ## merge dataframes after first one\n",
    "        if counter == 1:\n",
    "            df0 = df\n",
    "        else:\n",
    "\n",
    "            df0 = pd.merge(left=df0, right=df, how=\"left\", left_on=[\"datetime\"], right_on=[\"datetime\"])\n",
    "\n",
    "    df0.reset_index(inplace=True)\n",
    "\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "119d0fde-1a6e-4624-98de-12f9fb9a3f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = __read_LX_files(config['date'], config['path_to_LX_data'], threshold=config['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583fca1-a69b-4873-b2fa-a74a172f4d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### add column with sum of all sensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "39615923-9451-43f2-91e5-687ad8eccdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sum_all'] = df.sum(axis=1, numeric_only=True).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce9b9b-8c50-4700-90ea-d0c003bdaa3e",
   "metadata": {},
   "source": [
    "### write to daily files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bdd451a3-fe0f-4ca8-866d-e9966320e709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test if directory already exists\n",
    "if not os.path.isdir(config['path_to_outdata']+\"LXX.D/\"):\n",
    "    os.mkdir(config['path_to_outdata']+\"LXX.D/\")\n",
    "\n",
    "## format date\n",
    "fdate = config['date'].replace(\"-\", \"\")\n",
    "\n",
    "## write to daily pickle file\n",
    "df.to_pickle(config['path_to_outdata']+\"LXX.D/\"+f\"{fdate}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6f461-e8b2-462e-9314-9c9aa3655022",
   "metadata": {},
   "source": [
    "### add to overall maintenance log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8402125f-1229-4f5f-85ff-d336d8cbc3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df.sum_all != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "48820eb6-1998-4df2-ab6f-29fe9761c275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check if file already exists\n",
    "if not os.path.isfile(config['path_to_outlog']+\"LXX_maintenance.log\"):\n",
    "    df.to_csv(config['path_to_outlog']+\"LXX_maintenance.log\", mode='w', index=False, header=False)\n",
    "else:\n",
    "    df.to_csv(config['path_to_outlog']+\"LXX_maintenance.log\", mode='w', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebf56c-417c-4179-9a0c-e5312c60ede9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
