import os, sys
import obspy as obs
import matplotlib.pyplot as plt

from obspy.clients.fdsn import Client
from obspy.signal.trigger import coincidence_trigger
from obspy import UTCDateTime
from numpy import sort, argmin, array
from pandas import DataFrame, value_counts, merge, read_pickle

from functions.add_distances_and_backazimuth import __add_distances_and_backazimuth
from functions.catalog_to_dataframe import __catalog_to_dataframe


if os.uname().nodename == 'lighthouse':
    root_path = '/home/andbro/'
    data_path = '/home/andbro/kilauea-data/'
    archive_path = '/home/andbro/freenas/'
elif os.uname().nodename == 'kilauea':
    root_path = '/home/brotzer/'
    data_path = '/import/kilauea-data/'
    archive_path = '/import/freenas-ffb-01-data/'


config = {}

## old
# config['time_period'] = "20221001_20230615"

## new
config['time_period'] = "20221001_20230930"
config['time_period'] = "20221001_20231001"

config['BSPF_lon'] = array([-116.455439])
config['BSPF_lat'] = array([33.610643])

config['path_to_catalogs'] = data_path+"BSPF/data/catalogs/"

config['catalog_filename'] = f"BSPF_catalog_{config['time_period']}_all.xml"

config['output_filename'] = f"BSPF_catalog_{config['time_period']}_triggered.xml"

config['client'] = Client("IRIS")


get_ipython().getoutput("ls /import/kilauea-data/BSPF/data/catalogs/")


## read catalog
cat = obs.read_events(config['path_to_catalogs']+config['catalog_filename'])
cat.events.sort(key=lambda event: event.origins[0].time)


def __read_pickle(config, path, filename):

    import pickle

    with open(path+filename, 'rb') as f:
        triggerfile = pickle.load(f)

    return triggerfile


trigger_events = __read_pickle(config, config['path_to_catalogs'], "triggered_2022-10-01_2023-09-30.pkl")


times, cosum, times_rel, duration = [], [], [], []
ref_time = obs.UTCDateTime("2022-10-01")

for trig in trigger_events:
    times.append(trig['time'])
    cosum.append(trig['coincidence_sum'])
    times_rel.append((trig['time']-ref_time)/86400)
    duration.append(round(trig['duration'], 2))

trigger_times = sort(times)

coinc4 = [ev for ev in trigger_events if ev['coincidence_sum'] == 4]
coinc5 = [ev for ev in trigger_events if ev['coincidence_sum'] == 5]
coinc6 = [ev for ev in trigger_events if ev['coincidence_sum'] == 6]


df_trig = DataFrame()
df_trig['coincidence_sum'] = cosum
df_trig['trig_times_relative'] = times_rel
# df_trig['trigger_time'] = trigger_times
df_trig['trigger_time'] = trigger_times.astype(str)
df_trig['trigger_duration'] = duration

## remove duplicates of trigger time
df_trig = df_trig.drop_duplicates(["trigger_time"], keep="first").reset_index()

## remove trigger times before reference time
df_trig = df_trig[df_trig.trig_times_relative > 0]

trigger_times_unique = df_trig.trigger_time

print(f" removing duplicates")
print(f" before:  {len(trigger_times)}")
print(f" after:   {len(df_trig.trigger_time)}")


coinc0 = df_trig[df_trig.coincidence_sum >= 0]
coinc4 = df_trig[df_trig.coincidence_sum == 4]
coinc5 = df_trig[df_trig.coincidence_sum == 5]
coinc6 = df_trig[df_trig.coincidence_sum == 6]


print(f" total events:      {len(cat)}")
print(f" triggered events:  {len(trigger_events)}")
print(f" triggered events no duplicates:  {len(df_trig.trigger_time)}")

print(f" CoInc. = 4:        {len(coinc4)} ({round(len(coinc4)/len(coinc0)*100, 1)} %)")
print(f" CoInc. = 5:        {len(coinc5)} ({round(len(coinc5)/len(coinc0)*100, 1)} %)")
print(f" CoInc. = 6:        {len(coinc6)} ({round(len(coinc6)/len(coinc0)*100, 1)} %)")


def __expected_arrival_times(catalog, sta_lat, sta_lon):

    from obspy.geodetics import locations2degrees
    from obspy.taup import TauPyModel

    model = TauPyModel(model="ak135") ## "ak135"

    expected_times, expected_paths = [], []
    for jj, ev in enumerate(catalog):
        if jj == 300:
            break

        ev_lat = ev.origins[0].latitude
        ev_lon = ev.origins[0].longitude
        ev_dep = ev.origins[0].depth / 1000
        ev_time = ev.origins[0].time

        dist_deg = locations2degrees(sta_lat, sta_lon, ev_lat, ev_lon)

        try:
            arrivals = model.get_travel_times(source_depth_in_km=ev_dep,
                                              distance_in_degree=dist_deg,
                                              phase_list=["P","S","pP","sS"]
                                             )
            print(arrivals)
            expected_times.append(ev_time + arrivals[0].time)
        except:
            print(f" -> failed to compute arrival times")

        try:
            paths = model.get_ray_paths(source_depth_in_km=ev_dep,
                                        distance_in_degree=dist_deg,
                                        phase_list=["P","S","pP","sS"]
                                        )
            expected_paths.append(paths)

        except:
            print(f" -> failed to compute arrival paths")

    return expected_times, expected_paths


# expected_times, expected_paths = __expected_arrival_times(cat[:20], config['BSPF_lat'], config['BSPF_lon'])
# expected_times, expected_paths

# for path in expected_paths:
#     try:
#         ax = path.plot_rays(plot_type="cartesian")
#     except:
#         print("no")


def __find_matches(catalog, sta_lat, sta_lon, trig_times, max_time_offset):

    from obspy.geodetics import locations2degrees
    from obspy.taup import TauPyModel
    from obspy import UTCDateTime, Catalog
    from tqdm.notebook import tqdm

    ## model for arrival times
    model = TauPyModel(model="ak135") ## "ak135", "iasp91", "prem"

    ## counter for failed arrival computations
    fails = 0

    ## output dataframe
    df_out = DataFrame(columns=["id", "event_time", "arrival_time", "trigger_time", "arrivals_computed"])

    ## output catalog
    cat_out = Catalog()


    for jj, ev in enumerate(tqdm(catalog)):

#         if jj == 100:
#             break

        ## extract event data
        ev_lat = ev.origins[0].latitude
        ev_lon = ev.origins[0].longitude
        ev_dep = ev.origins[0].depth / 1000
        ev_time = ev.origins[0].time

        ## compute distance between station to event in degrees
        dist_deg = locations2degrees(sta_lat, sta_lon, ev_lat, ev_lon)

        # print(dist_deg[0], ev_dep)

        try:
            arrivals = model.get_travel_times(source_depth_in_km=ev_dep,
                                              distance_in_degree=dist_deg[0],
                                              # phase_list=["P","S","pP","sS","sP","pS"]
                                              phase_list=["P"]
                                             )
            # print(arrivals)
            expected_time = ev_time + arrivals[0].time
            arrivals_computed = 1

        except Exception as e:
            # print(e)
            fails += 1
            expected_time = ev_time
            arrivals_computed = 0


        for nn, trig in enumerate(trig_times):

            ## compute absolute temporal distance
            abs_diff = abs(expected_time - UTCDateTime(trig))

            # diff = expected_time - UTCDateTime(trig)

            ## if distance below threshold, trigger is considered match with catalog event
            if abs_diff < max_time_offset:
                df_out.loc[len(df_out)] = [jj, ev_time, expected_time, trig, arrivals_computed]
                cat_out += ev
            else:
                continue


    print(f" -> failed for {fails} of {len(catalog)} to compute arrival times..:")

    return df_out, cat_out


df_out, cat_select = __find_matches(cat, config['BSPF_lat'], config['BSPF_lon'], trigger_times_unique, max_time_offset=10)


df_all = merge(df_trig, df_out, on=['trigger_time'])
df_all


df_all[df_all.arrivals_computed == 1].size


## list of events to reject (manually)
no_event = ["2022-10-05 15:39:13.690",
            "2022-10-05 15:42:03.610",
            "2022-10-05 18:35:16.860",
            "2022-10-10 05:48:52.890",
            "2022-10-12 19:12:14.670",
            "2022-10-12 23:08:11.410",
            "2022-10-12 23:58:00.800",
            "2022-10-14 03:13:56.380",
            "2022-10-16 08:42:24.970",
            "2022-10-26 08:27:09.860",
            "2022-10-28 12:47:26.910",
            "2022-11-03 05:53:34.230",
            "2022-11-09 00:46:57.550",
            "2022-11-10 16:53:29.940",
            "2022-11-25 10:45:36.390",
            "2022-12-06 21:56:31.330",
            "2022-12-09 06:03:03.220",
            "2022-12-15 13:55:13.460",
            "2022-12-17 18:02:34.910",
            "2022-12-19 15:41:22.380",
            "2022-12-19 15:41:26.020",
            "2023-01-05 09:59:04.210",
            "2023-01-07 12:32:45.250",
            "2023-01-16 00:00:36.100",
            "2023-01-29 23:50:47.770",
            "2023-02-05 23:38:46.670",
            "2023-02-22 05:09:39.970",
            "2023-03-09 11:49:06.640",
            "2023-03-17 07:12:56.970",
            "2023-03-24 13:59:00.160",
            "2023-03-29 19:23:29.470",
            "2023-03-29 19:23:27.460",
            "2023-04-30 07:26:56.390",
            "2023-04-06 02:42:00.600",
            "2023-04-13 23:57:53.950",
            "2023-04-17 15:27:56.450",
            "2023-05-01 02:52:56.160",
            "2023-06-02 20:19:46.740",
            "2023-06-05 07:39:16.650",
            "2023-09-11 23:57:56.280", ## to small for the distance
            "2022-10-26 08:14:33.950", ## dubplicate trigger time
            "2022-11-30 14:05:03.530", ## dubplicate trigger time
            "2022-12-05 10:55:03.520", ## dubplicate trigger time
            "2023-02-01 09:19:10.710", ## dubplicate trigger time
            "2023-03-26 07:47:55.340", ## dubplicate trigger time
            "2023-03-26 07:47:59.500", ## dubplicate trigger time
            "2023-04-10 14:50:54.400", ## dubplicate trigger time
            "2023-04-30 07:27:20.170", ## dubplicate trigger time
            "2023-05-09 03:01:09.100", ## dubplicate trigger time
            "2023-06-05 17:22:15.270", ## dubplicate trigger time
            "2023-06-28 01:45:06.060", ## dubplicate trigger time
            "2023-07-10 20:53:23.380", ## dubplicate trigger time
            "2023-07-12 02:29:00.340", ## dubplicate trigger time
            "2023-07-24 18:18:36.100", ## dubplicate trigger time
           ]


no_event_utc = list(map(UTCDateTime, no_event))

n_before = len(df_all)

## remove defined events from data frame
df_all = df_all[df_all['event_time'].apply(lambda x: x not in no_event_utc)].reset_index()

## remove defined events from catalog
new_cat = obs.Catalog()
for tmp_ev in cat_select:
    if tmp_ev.origins[0].time not in no_event_utc:
        new_cat += tmp_ev

cat_select = new_cat


print(f" before:  {n_before} \n after:   {len(df_all)} \n manuall list: {len(no_event_utc)}  \n rejected: {n_before-len(df_all)}")


dates = [str(cat_select[ii].origins[0].time.date) for ii in range(len(cat_select))]

df = DataFrame()
df['dates'] = dates
df = df.sort_values(by="dates")

df2 = df.apply(value_counts)
df2.sort_index(inplace=True)

df2.dates.plot(kind='bar', subplots=True, figsize=(15, 5))


cat_select.write(config['path_to_catalogs']+config['output_filename'], format="QUAKEML")
print(f" -> storing: {config['path_to_catalogs']}{config['output_filename']} ... ")


cata_df = __catalog_to_dataframe(cat_select)


## create a data frame for the triggered catalog
cata_df = cata_df.rename_axis('origin').reset_index()
cata_df['seconds'] = [abs(UTCDateTime(str(tt))-UTCDateTime("2022-10-01")) for tt in cata_df.origin]
cata_df['trigger_time'] = df_all.trigger_time
cata_df['arrival_time'] = df_all.arrival_time
cata_df['event_time'] = df_all.event_time
cata_df['cosum'] = df_all.coincidence_sum


## add information on epicentral distance and backazimuths
cata_df = __add_distances_and_backazimuth(config['BSPF_lat'], config['BSPF_lon'], cata_df)

## remove events where magnitude is smaller than 1 and distance is larger than 15km
cata_df.drop(cata_df[(cata_df.magnitude < 1) & (cata_df.distances_km > 15)].index, inplace=True)
cata_df.reset_index(drop=True)

## remove duplicated trigger times and event times
# cata_df = cata_df.drop_duplicates("trigger_time", keep="first").reset_index(drop=True)
cata_df = cata_df.drop_duplicates("event_time").reset_index(drop=True)


## write data to output file
cata_df.to_pickle(config['path_to_catalogs']+config['output_filename'].replace(".xml",".pkl"))
print(f" -> storing: {config['path_to_catalogs']}{config['output_filename'].replace('.xml','.pkl')}...")


cata_df


print("number of events: ", len(cata_df.origin))
print("max. magnitude: ", max(cata_df.magnitude))
print("min. magnitude: ", min(cata_df.magnitude))

print("max. distance: ", round(max(cata_df.distances_km),2), "km")
print("min. distance: ", round(min(cata_df.distances_km),2), "km")


cata_df


print(len(cata_df[cata_df.magnitude < 1]), cata_df.magnitude.min())
cata_df[cata_df.magnitude < 1]



cata_df[cata_df.duplicated("event_time", keep=False)]
cata_df[cata_df.duplicated("trigger_time", keep=False)]


## remove clearly visible outlier on 2023-04-30T07:26:56.390000Z
# cata_df[(cata_df.seconds > 86400*210) & (cata_df.seconds < 86400*215)]

# print(cat.filter("time > 2023-04-30T06:10","time < 2023-04-30T08:20", "magnitude > 2", inverse=False).__str__(print_all=True))


cata_df[cata_df.magnitude > 3]


good_event = [
            "20221002_032516",
            "20221015_051734",
            "20221023_092458",
            "20221026_081433", ## double event
            "20221231_121226",
            "20230109_194256",
            "20230301_224903",
            "20230308_040732",
            "20230410_145054",
            "20230417_185236",
            "20230425_115547",
            "20230430_221115",
            "20230707_160229",
            "20230828_102734",
            "20230830_153205",
            "20230911_092015",
            "20230919_003513"
           ]


## get indices of good events
idx = []
for ge in good_event:
    for i, ev in zip(cata_df.index, cata_df.origin):
        ev_str = str(ev).split(".")[0].replace(" ", "_").replace("-","").replace(":","")
        if ge == ev_str:
            idx.append(i)

## extract good events as separate catalog
cata_good = cata_df[cata_df.index.isin(idx)].reset_index().rename(columns={"index":"event_number"})

## write data to output file
cata_good.to_pickle(config['path_to_catalogs']+f"good_events_{config['time_period']}.pkl")
print(f" -> storing: {config['path_to_catalogs']}good_events_{config['time_period']}.pkl ...")








cata_df[cata_df.distances_km > 100]
