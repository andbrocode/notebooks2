{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Projection Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this exercise, we are using the array data from north America to preform back-projection and tract the rupture process of the 2020-10-19 Mw 7.6 Alaska earthquake. The miniseed data is downloaded from IRIS website using the following link: http://ds.iris.edu/wilber3/find_stations/11327190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import necessary libraries and functions\n",
    "from pydsm.relab import shiftdim\n",
    "import numpy as np\n",
    "from obspy import read, UTCDateTime, read_inventory, Inventory\n",
    "import os\n",
    "from obspy.clients.fdsn import Client, RoutingClient\n",
    "from obspy.taup import TauPyModel\n",
    "from obspy.geodetics import locations2degrees\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save calculation time, here we only select stations in a relatively short distance ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Station selection\n",
    "#distance range 40-50\n",
    "#azimuth range 60-140\n",
    "#channel BHZ\n",
    "\n",
    "\n",
    "eq='Alaska'\n",
    "\n",
    "duration=15*60\n",
    "freq_resampling=10\n",
    "taupModel='ak135'\n",
    "#Pdur=20\n",
    "minCrossCorrelation=0.6\n",
    "freq_output=1.0\n",
    "BP_threshold_event = 0.15\n",
    "freqmin_select=0.1\n",
    "freqmax_select=1.\n",
    "aSlidingWindowDuration=3.0 ##time in seconds\n",
    "aSlidingWindowStep = 1\n",
    "\n",
    "\n",
    "fndata='2020-10-19-mww76-south-of-alaska.miniseed'\n",
    "hypolon, hypolat, hypo_depth_in_km = -159.652, 54.609 ,31.08\n",
    "grid_depth_in_km=31.08 # from wilber\n",
    "sStarttime = \"2020-10-19 20:54:39\"\n",
    "freqs_BP=[[0.5, 1.0]]\n",
    "\n",
    "##For the first time, ususally select a larger region to make sure the rupture is within it. Then you can use a smaller\n",
    "##one and refined grids\n",
    "\n",
    "eventlat=np.arange(52.5,56.5,0.1)\n",
    "eventlon=np.arange(-163.0,-155.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##functions to remove instrument response, plot array data, do array cross-correlation, and time shift \n",
    "def get_instrument_response(st, client, station_coords):\n",
    "    #get instrument response\n",
    "    ntrace = len(st)\n",
    "    for i,tr in enumerate(st):\n",
    "        fn='inventory/%s.%s.xml' %(tr.stats.network, tr.stats.station)\n",
    "        if os.path.isfile(fn):\n",
    "            inv= read_inventory(fn)\n",
    "        else:\n",
    "            try:\n",
    "               inv = client.get_stations(network=tr.stats.network, station=tr.stats.station,\n",
    "                                       starttime=starttime,\n",
    "                                       endtime=endtime, \n",
    "                                       level=\"response\")\n",
    "               inv.write(fn, format=\"STATIONXML\")\n",
    "               print('response for station %s.%s downloaded (%d/%d)' %(tr.stats.network, tr.stats.station, i, ntrace))\n",
    "            except:\n",
    "               print('no response data for trace', tr)\n",
    "               st.remove(tr)\n",
    "               continue\n",
    "        try:\n",
    "           tr.attach_response(inv)\n",
    "        except:\n",
    "           print('could not attach response data for trace', tr)\n",
    "           st.remove(tr)\n",
    "           continue\n",
    "        code = '%s.%s' %(inv[0].code, inv[0][0].code)\n",
    "        station_coords[code]= [inv[0][0].longitude, inv[0][0].latitude]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveResponseAndReturnCoords(st):\n",
    "    st.merge()\n",
    "    station_coords = {}\n",
    "    get_instrument_response(st, client, station_coords)\n",
    "    print('%d stations in file' %(len(station_coords)))\n",
    "    #save all stations for latter plotting\n",
    "    with open(StorePrefix+'station_coords_all.pkl', 'wb') as f:\n",
    "       pickle.dump(station_coords, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    pre_filt = (0.01, 0.05, 10, 20)\n",
    "    for i,tr in enumerate(st):\n",
    "        try:\n",
    "            tr.remove_response(output='VEL', pre_filt=pre_filt)\n",
    "        except ValueError:\n",
    "            st.remove(tr)\n",
    "    for i,tr in enumerate(st):\n",
    "        if np.max(np.abs(tr.data))>1e-2:\n",
    "            st.remove(tr)\n",
    "    return station_coords\n",
    "\n",
    "\n",
    "def plotarraydata(st,scal): ## scal scaling/normalizing factor for seismogram\n",
    "    for istation in range(len(st)):\n",
    "        tmp = st[istation].data\n",
    "        if len(tmp)==0:\n",
    "            print('no data for station:', st[istation].stats.station)\n",
    "        else:\n",
    "            if scal >0:\n",
    "                tmp1 = tmp/(np.max(tmp)-np.min(tmp))\n",
    "            else:\n",
    "                tmp1 = tmp\n",
    "            t = np.arange(0,len(tmp1),1)*st[istation].stats.delta\n",
    "            plt.plot(t,tmp1*np.abs(scal)-istation*np.abs(scal),'k')\n",
    "            plt.xlabel('Time (s)',fontsize=15)\n",
    "            \n",
    "def doarraycrosscorrelation(st,st_time,duration,PointsAllowShift):\n",
    "    start_point=np.int(st_time/st[0].stats.delta)\n",
    "    point_len=np.int(duration/st[0].stats.delta)\n",
    "    aCrossCorrelationShift=np.zeros((len(st), len(st)))\n",
    "    aCrossCorrelationValue=np.zeros((len(st), len(st)))\n",
    "    relativepolarization=np.zeros((len(st), len(st)))\n",
    "    for i in range(len(st)):\n",
    "        for j in range(len(st)):\n",
    "            cc = correlate(st[i].data[start_point:start_point+point_len], st[j].data[start_point:start_point+point_len], PointsAllowShift)\n",
    "            shift, value = xcorr_max(np.abs(cc))  ##absolute cc value\n",
    "            aCrossCorrelationShift[i,j]=shift\n",
    "            aCrossCorrelationValue[i,j]=value\n",
    "            relativepolarization[i,j] = cc[np.abs(cc).argmax()]/value\n",
    "    return aCrossCorrelationShift, aCrossCorrelationValue , relativepolarization\n",
    "\n",
    "def dotimeshift(st,timematrix): ##timemaxtrix have same length as station\n",
    "    st_shift=st.copy()\n",
    "    for i in range(len(st)):\n",
    "        delay = timematrix[i] ##compute lag from slownesses \n",
    "        rep = np.exp(1j*2*np.pi*frq*delay)\n",
    "        vehlp = np.fft.fft(st_shift[i].data)\n",
    "        shft = shiftdim(vehlp[0:len(frq)],1,nargout=1)\n",
    "        dshft = rep.T*shft\n",
    "        s_sh = np.double(2*np.real(np.fft.ifft(dshft, Nfft)))\n",
    "        st_shift[i].data=s_sh\n",
    "    return st_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "for myfold in ['inventory', 'data']:\n",
    "    if not os.path.exists(myfold):\n",
    "       os.mkdir(myfold)\n",
    "\n",
    "\n",
    "starttime = UTCDateTime(sStarttime)\n",
    "endtime=starttime+duration\n",
    "#where to save data\n",
    "StorePrefix = 'data/'+ eq + '_'+os.path.basename(os.path.splitext(fndata)[0])\n",
    "\n",
    "st = read(fndata)\n",
    "print('removing response...')\n",
    "station_coords = RemoveResponseAndReturnCoords(st)\n",
    "\n",
    "#remove duplicated stations\n",
    "aCodes = []\n",
    "for i,tr in enumerate(st):\n",
    "    code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "    if code in aCodes:\n",
    "       st.remove(tr)\n",
    "    else:\n",
    "       aCodes.append(code)\n",
    "\n",
    "st.detrend('demean')\n",
    "st.detrend()\n",
    "\n",
    "\n",
    "st_ref=st.copy() ##save a copy of unfiltered data\n",
    "\n",
    "st.filter('bandpass', freqmin=freqmin_select, freqmax=freqmax_select, corners=4, zerophase=True)\n",
    "st.resample(freq_resampling)\n",
    "st.trim(starttime, endtime, pad=True, fill_value=0)\n",
    "\n",
    "# Test if some trace have nan data or no signal (all 0)\n",
    "removed=[]\n",
    "for ist,tr in enumerate(st):\n",
    "   if np.any(np.isnan(tr.data)) or abs(tr.max())==0.0:\n",
    "      code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "      removed.append(code)\n",
    "      st.remove(tr)\n",
    "print('removed because nan or 0:', removed)\n",
    "if len(removed)>0:\n",
    "   for ist,tr in enumerate(st_ref):\n",
    "      code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "      if code in removed:\n",
    "         st_ref.remove(tr)\n",
    "\n",
    "print(st.__str__(extended=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the original/unshifted time recordings\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "plotarraydata(st,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove clustered stations\n",
    "removed=[]\n",
    "for i,tr in enumerate(st):\n",
    "    if (i==0):\n",
    "        code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "        st_loc = [[station_coords[code][1], station_coords[code][0]]]\n",
    "    else:\n",
    "        code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "        loc1 = [[station_coords[code][1], station_coords[code][0]]]\n",
    "        dis = np.zeros((len(st_loc),1))\n",
    "        for ind in range(len(st_loc)):\n",
    "            dis[ind]=locations2degrees(lat1= loc1[0][0], long1= loc1[0][1], lat2 = st_loc[ind][0], long2 =st_loc[ind][1] )\n",
    "        if (np.min(dis)<0.3):\n",
    "            print(np.min(dis))  \n",
    "            removed.append(code)\n",
    "            st.remove(tr)\n",
    "        else:\n",
    "            st_loc += loc1\n",
    "print('removed because too close space interval:', removed)\n",
    "if len(removed)>0:\n",
    "   for ist,tr in enumerate(st_ref):\n",
    "      code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "      if code in removed:\n",
    "         st_ref.remove(tr)\n",
    "print(st.__str__(extended=True))\n",
    "plotarraydata(st,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we use Taup to calculate the theoretical travel time from the hypo to the array. Later we'll calculate the travel time between all potential source grids to the array. More info can be found at: https://docs.obspy.org/packages/obspy.taup.html#module-obspy.taup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TheoreticalTTime = np.zeros((len(st)))\n",
    "model = TauPyModel(model=taupModel)\n",
    "for i,tr in enumerate(st):\n",
    "    code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "    dist = locations2degrees(lat1= station_coords[code][1], long1= station_coords[code][0], lat2 = hypolat, long2 = hypolon )\n",
    "    TheoreticalTTime[i] = model.get_travel_times(source_depth_in_km=hypo_depth_in_km, distance_in_degree=dist, phase_list=[\"P\"])[0].time\n",
    "\n",
    "meTheoreticalTTime = np.mean(TheoreticalTTime)\n",
    "TheoreticalTTimeRelativeToMean = TheoreticalTTime - meTheoreticalTTime\n",
    "\n",
    "##all waveforms resampled to the same sampling rate\n",
    "dt = st[0].stats.delta  ## sampling interval\n",
    "Fmax=1/dt ## maximum frequency\n",
    "Nfft = st[0].stats.npts ## number of sample in each chopped seismogram (i.e. number in fft)\n",
    "frq = Fmax/2*np.linspace(0,1,endpoint=True, num=np.int(Nfft/2+1)); ## central frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Seismograms shifted with empirical travel time from the hypocenter\n",
    "st_shift_hypo=dotimeshift(st,TheoreticalTTimeRelativeToMean)\n",
    "plotarraydata(st_shift_hypo,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TheoreticalTTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use cross-correlation to do time calibration to the hypo location\n",
    "aCrossCorrelationShift, aCrossCorrelationValue , relativepolarization = doarraycrosscorrelation(st_shift_hypo,485,25,200)\n",
    "plt.imshow(aCrossCorrelationValue, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "avStationaCrossCorrelation=np.mean(aCrossCorrelationValue, axis=0)\n",
    "print(avStationaCrossCorrelation)\n",
    "\n",
    "##find the index of the station with maximum average array cross-correlation coefficients\n",
    "index_ref = avStationaCrossCorrelation.argmax()\n",
    "T_cross_all = aCrossCorrelationShift[index_ref,:]*st[0].stats.delta\n",
    "Polar_cross_all = relativepolarization[index_ref,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T_cross_all)\n",
    "print(Polar_cross_all) ##For a large array, it can be in two quadrants relative to the event focal mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##time shift for cross-correlation time\n",
    "T_cross_all = -1* T_cross_all\n",
    "st_shift_hypo_cross=dotimeshift(st_shift_hypo,T_cross_all)\n",
    "plotarraydata(st_shift_hypo_cross,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##find the stations with coherent signal above the threshold\n",
    "index_select = np.where(avStationaCrossCorrelation>minCrossCorrelation)[0]\n",
    "\n",
    "T_cross = T_cross_all[index_select]\n",
    "Polarization = Polar_cross_all[index_select]\n",
    "for i,tr in enumerate(st):\n",
    "    if not i in index_select:\n",
    "        st.remove(tr)\n",
    "\n",
    "for i,tr in enumerate(st_ref):\n",
    "    if not i in index_select:\n",
    "        st_ref.remove(tr)\n",
    "\n",
    "\n",
    "station_coords_selected = {}\n",
    "for i,tr in enumerate(st):\n",
    "    code = '%s.%s' %(tr.stats.network, tr.stats.station)\n",
    "    station_coords_selected[code] = station_coords[code]\n",
    "\n",
    "#Write data to file\n",
    "st_ref.write(StorePrefix+'selected.mseed', format=\"MSEED\")\n",
    "with open(StorePrefix+'station_coords.pkl', 'wb') as f:\n",
    "    pickle.dump(station_coords_selected, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('T_cross.pkl', 'wb') as f:\n",
    "    pickle.dump(T_cross, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('Polarization.pkl', 'wb') as f:\n",
    "    pickle.dump(Polarization, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##start calculate the travel between target region grids to array\n",
    "eventlat=np.arange(52.5,56.5,0.1)\n",
    "eventlon=np.arange(-163.0,-155.9,0.1)\n",
    "nx=len(eventlon)\n",
    "ny=len(eventlat)\n",
    "eventdep = 31\n",
    "nstations=len(station_coords_selected)\n",
    "taupModel='ak135'\n",
    "#theoreticalTTime_SrcGrid_StaArray = np.zeros((nx, ny, nstations))\n",
    "\n",
    "model = TauPyModel(model=taupModel)\n",
    "\n",
    "coords = list(station_coords_selected.values())\n",
    "print(coords)\n",
    "print(len(coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_loc = np.zeros((len(coords),2))\n",
    "for istation in range(len(coords)):\n",
    "    sta_loc[istation,0] = coords[istation][0]\n",
    "    sta_loc[istation,1] = coords[istation][1]             \n",
    "plt.plot(sta_loc[:,0],sta_loc[:,1],'o')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(num=None, figsize=(12, 8) )\n",
    "m = Basemap(projection='merc',llcrnrlat=np.min(sta_loc[:,1])-1,urcrnrlat=np.max(sta_loc[:,1])+1,llcrnrlon=np.min(sta_loc[:,0])-1,urcrnrlon=np.max(sta_loc[:,0])+1,resolution='l')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='tan',lake_color='lightblue')\n",
    "m.drawparallels(np.arange(np.min(eventlat),np.max(eventlat),1),labels=[True,True,False,False],dashes=[2,2])\n",
    "m.drawmeridians(np.arange(np.min(eventlon),np.max(eventlon),1),labels=[False,False,False,True],dashes=[2,2])\n",
    "m.drawmapboundary(fill_color='lightblue')\n",
    "stalon_basemap,stalat_basemap = m(sta_loc[:,0],sta_loc[:,1])\n",
    "plt.plot(stalon_basemap,stalat_basemap,'o')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeTravelTime(gridlon,gridlat,griddepth,stationcoords,taupmodel):\n",
    "    nx=len(gridlon)\n",
    "    ny=len(gridlat)\n",
    "    nstations=len(stationcoords)\n",
    "    model = TauPyModel(model=taupModel)\n",
    "    TheoreticalTravelTime = np.zeros((nx, ny, nstations))\n",
    "    for ilon in range(nx):\n",
    "        for ilat in range(ny):\n",
    "            for istation in range(nstations):\n",
    "                dis_in_deg = locations2degrees(lat1= stationcoords[istation][1], long1= stationcoords[istation][0], lat2 = eventlat[ilat], long2 = eventlon[ilon] ) \n",
    "                TheoreticalTravelTime[ilon,ilat,istation] = model.get_travel_times(source_depth_in_km=griddepth, distance_in_degree=dis_in_deg, phase_list=[\"P\"])[0].time\n",
    "    return TheoreticalTravelTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check the calculation time\n",
    "##Skip this if you have already done this & there is no change for the target grids and stations list\n",
    "import timeit\n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "TheoreticalTravelTime = ComputeTravelTime(eventlon,eventlat,eventdep,coords,taupModel)\n",
    "\n",
    "\n",
    "stoptime = timeit.default_timer()\n",
    "print('Running Time: ', stoptime - starttime) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save the travel time matrix\n",
    "with open('TheoreticalTravelTime.pkl', 'wb') as f:\n",
    "    pickle.dump(TheoreticalTravelTime, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fndata= StorePrefix +'selected.mseed'\n",
    "\n",
    "starttime = UTCDateTime(sStarttime)\n",
    "endtime=starttime+duration\n",
    "st = read(fndata, format=\"MSEED\")\n",
    "st.taper(max_percentage=0.05)\n",
    "st.filter('bandpass', freqmin= freqs_BP[0][0], freqmax=freqs_BP[0][1], corners=4, zerophase=True)\n",
    "st.resample(freq_resampling)\n",
    "st.trim(starttime, endtime, pad=True, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotarraydata(st,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TheoreticalTravelTime.shape)\n",
    "print(st.__str__(extended=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TheoreticalTravelTime = pickle.load( open('TheoreticalTravelTime.pkl', \"rb\" ) )\n",
    "Time_matrix = np.copy(TheoreticalTravelTime)\n",
    "T_cross = pickle.load( open('T_cross.pkl', \"rb\" ) )\n",
    "Polarization = pickle.load( open('Polarization.pkl', \"rb\" ) )\n",
    "average_TheoreticalTravelTime = np.average(TheoreticalTravelTime, axis=2)\n",
    "for ist,tr in enumerate(st):\n",
    "      Time_matrix[:,:,ist] = Time_matrix[:,:,ist] - average_TheoreticalTravelTime\n",
    "print(T_cross)\n",
    "print(Polarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movsum(array,k):\n",
    "    rollsum = np.zeros(len(array)-k+1)\n",
    "    for i in range(len(array)-k+1):\n",
    "        rollsum[i] = np.sum(array[i:i+k])\n",
    "    return rollsum\n",
    "##all waveforms resampled to the same sampling rate\n",
    "dt = st[0].stats.delta  ## sampling interval\n",
    "Fmax=1/dt ## maximum frequency\n",
    "Nfft = st[0].stats.npts ## number of sample in each chopped seismogram (i.e. number in fft)\n",
    "frq = Fmax/2*np.linspace(0,1,endpoint=True, num=np.int(Nfft/2+1)); ## central frequencies  \n",
    "\n",
    "\n",
    "nx=len(eventlon)\n",
    "ny=len(eventlat)\n",
    "win_length= aSlidingWindowDuration #in seconds\n",
    "win_step = aSlidingWindowStep #in seconds\n",
    "win_sp = np.int(win_length/dt)\n",
    "step_sp = np.int(win_step/dt)\n",
    "mc=np.arange(0,Nfft-2*win_sp,step_sp)\n",
    "energy=np.zeros((nx, ny, len(mc)))\n",
    "print(energy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "starttime = timeit.default_timer()\n",
    "st_bp = st.copy()\n",
    "for ilon in range(1):\n",
    "    for ilat in range(1):\n",
    "        stdel = st_bp[0].data*0\n",
    "        for ista in range(len(st)):\n",
    "            delay = Time_matrix[ilon,ilat,ista]+ T_cross[ista]\n",
    "            rep = np.exp(1j*2*np.pi*frq*delay)\n",
    "            vehlp = np.fft.fft(st_bp[ista].data)\n",
    "            shft = shiftdim(vehlp[0:len(frq)],1,nargout=1)\n",
    "            dshft = rep.T*shft\n",
    "            s_sh = np.double(2*np.real(np.fft.ifft(dshft, Nfft)))\n",
    "            stdel = stdel + s_sh/np.max(np.abs(s_sh))*Polarization[ista]\n",
    "        stdel = stdel **2\n",
    "        stdel_sum=movsum(stdel,step_sp)\n",
    "        print(stdel_sum.shape)\n",
    "        plt.plot(stdel_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "starttime = timeit.default_timer()\n",
    "st_bp = st.copy()\n",
    "for ilon in range(nx):\n",
    "    for ilat in range(ny):\n",
    "        stdel = st[0].data*0\n",
    "        for ista in range(len(st)):\n",
    "            delay = Time_matrix[ilon,ilat,ista]+ T_cross[ista]\n",
    "            rep = np.exp(1j*2*np.pi*frq*delay)\n",
    "            vehlp = np.fft.fft(st[ista].data)\n",
    "            shft = shiftdim(vehlp[0:len(frq)],1,nargout=1)\n",
    "            dshft = rep.T*shft\n",
    "            s_sh = np.double(2*np.real(np.fft.ifft(dshft, Nfft)))\n",
    "            stdel = stdel + s_sh/np.max(np.abs(s_sh))*Polarization[ista]\n",
    "        stdel = stdel **2\n",
    "        stdel_sum=movsum(stdel,win_sp)\n",
    "        energy[ilon,ilat,:] = stdel_sum[mc]\n",
    "        \n",
    "stoptime = timeit.default_timer()\n",
    "print('Running Time: ', stoptime - starttime) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('energy.pkl', 'wb') as f:\n",
    "    pickle.dump(energy, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beampeak = np.zeros((len(mc)))\n",
    "beampeak_lon = np.zeros((len(mc)))\n",
    "beampeak_lat = np.zeros((len(mc)))\n",
    "for inum in range(len(mc)):\n",
    " \n",
    "    beam = np.zeros((nx,ny))\n",
    "    beam = energy[:,:,inum]\n",
    "    \n",
    "    # find peak\n",
    "    indextuple = np.unravel_index(np.argmax(beam, axis=None), beam.shape)\n",
    "    beampeak[inum] = beam[indextuple]\n",
    "    ix = indextuple[0]\n",
    "    iy = indextuple[1]\n",
    "    \n",
    "    beampeak_lon[inum] = eventlon[ix]\n",
    "    beampeak_lat[inum] = eventlat[iy]\n",
    "plt.plot(beampeak)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Beam power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##select the time windows for potential ruptures\n",
    "n_pick = np.arange(496,540,1)\n",
    "t=np.arange(0,len(n_pick),1)*win_step\n",
    "mg = beampeak[n_pick]\n",
    "mg = mg/np.max(mg)\n",
    "plt.plot(t,mg)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Normalized Beampower')\n",
    "\n",
    "lon_pick = beampeak_lon[n_pick]\n",
    "lat_pick = beampeak_lat[n_pick]\n",
    "print(lat_pick,lon_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##time cali for grids location relative to the hypo\n",
    "dist_hypo = locations2degrees(lat1= np.mean(sta_loc[:,1]), long1= np.mean(sta_loc[:,0]), lat2 = hypolat, long2 = hypolon )\n",
    "T_hypo = model.get_travel_times(source_depth_in_km=hypo_depth_in_km, distance_in_degree=dist_hypo, phase_list=[\"P\"])[0].time\n",
    "print(T_hypo)\n",
    "dis = t*0;\n",
    "T_grid = t*0\n",
    "for ind in range(len(n_pick)):\n",
    "    print(lon_pick[ind],lat_pick[ind])\n",
    "    dist = locations2degrees(lat1= np.mean(sta_loc[:,1]), long1= np.mean(sta_loc[:,0]), lat2 = lat_pick[ind], long2 = lon_pick[ind] )\n",
    "    T_grid[ind] = model.get_travel_times(source_depth_in_km=hypo_depth_in_km, distance_in_degree=dist, phase_list=[\"P\"])[0].time\n",
    "print(T_grid)\n",
    "dt = T_grid - T_hypo;\n",
    "\n",
    "print(dt+t)\n",
    "plt.plot(t,mg)\n",
    "plt.plot(dt+t,mg,'r.')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('Normalized Beampower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('brg')\n",
    "plt.scatter(lon_pick,lat_pick, mg*100, t+dt, alpha = 0.5, vmin=0, vmax=np.max(t), cmap= cm)\n",
    "plt.colorbar()\n",
    "plt.plot(hypolon, hypolat, 'rp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inum = 496\n",
    "beam = np.zeros((nx,ny))\n",
    "beam = energy[:,:,inum]\n",
    "print(beam.shape)\n",
    "indextuple = np.unravel_index(np.argmax(beam, axis=None), beam.shape)\n",
    "\n",
    "ix = indextuple[0]\n",
    "iy = indextuple[1]\n",
    "print(ix)\n",
    "print(iy)\n",
    "print(eventlon[ix])\n",
    "\n",
    "plt.imshow(beam.T,origin='lower',extent=[-163,-155.9,52.5,56.5], aspect = 1/np.cos(np.mean(eventlat)/180*np.pi))\n",
    "plt.colorbar()\n",
    "plt.plot(hypolon, hypolon, 'rp')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('BP in time: '+str(round(inum*win_step,1))+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(n_pick)):\n",
    "    inum = n_pick[i]\n",
    "    beam = np.zeros((nx,ny))\n",
    "    beam = energy[:,:,inum]\n",
    "    plt.imshow(beam.T,origin='lower',extent=[-163,-155.9,52.5,56.5], aspect = 1/np.cos(np.mean(eventlat)/180*np.pi))\n",
    "    plt.colorbar()\n",
    "    plt.plot(hypolon, hypolon, 'rp')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('BP in time: '+str(round(inum*win_step,1))+' s')\n",
    "    plt.savefig('BP in'+str(round(inum*win_step,1))+' s.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "fig = plt.figure(num=None, figsize=(12, 8) )\n",
    "m = Basemap(projection='merc',llcrnrlat=np.min(eventlat),urcrnrlat=np.max(eventlat),llcrnrlon=np.min(eventlon),urcrnrlon=np.max(eventlon),resolution='h')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='tan',lake_color='lightblue')\n",
    "m.drawparallels(np.arange(np.min(eventlat),np.max(eventlat),1),labels=[True,True,False,False],dashes=[2,2])\n",
    "m.drawmeridians(np.arange(np.min(eventlon),np.max(eventlon),1),labels=[False,False,False,True],dashes=[2,2])\n",
    "m.drawmapboundary(fill_color='lightblue')\n",
    "cm = plt.cm.get_cmap('brg')\n",
    "lon_basemap,lat_basemap = m(lon_pick,lat_pick)\n",
    "plt.scatter(lon_basemap,lat_basemap, mg*100, t+dt, alpha = 0.5, vmin=0, vmax=np.max(t), cmap= cm)\n",
    "plt.colorbar()\n",
    "hypo_lon,hypo_lat = m(hypolon, hypolat)\n",
    "plt.plot(hypo_lon, hypo_lat, 'rp')\n",
    "plt.title(\"BP rupture locations\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
